{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fetching The Data from Twitter API</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter API Keys\n",
    "consumerKey = \"5aUYLw0kl8aug0ReGxtdPabZN\"\n",
    "consumerSecret = \"aJHj7FMeA5adpZC1Ixbq2X7ObW0qOJx4e7URYX2WgA0j76fDFJ\"\n",
    "accessToken = \"1384610198078230531-UG3OwzfuXUmcCnY4mKsIdkTlTvY63A\"\n",
    "accessTokenSecret = \"for1lHlL1xU8Gz1bHuUHXanYKWdS3MnbX1liHUToRrdkR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the authentication object\n",
    "authenticate = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "#Set the access token and access token secret\n",
    "authenticate.set_access_token(accessToken, accessTokenSecret)\n",
    "#Create the API object while passing the auth information\n",
    "api = tweepy.API(authenticate, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_datetime</th>\n",
       "      <th>tweet_Id</th>\n",
       "      <th>Twitter_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @TaraBull808: @CEOAdam I'd love to invite y...</td>\n",
       "      <td>2021-05-07 01:22:07</td>\n",
       "      <td>1390477009751482376</td>\n",
       "      <td>jamiegagnon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Dogecoinrich: What is your #Dogecoin price...</td>\n",
       "      <td>2021-05-07 01:22:07</td>\n",
       "      <td>1390477008694431746</td>\n",
       "      <td>JustStocksToday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Come check us out, live right now! \\n\\nWe LOVE...</td>\n",
       "      <td>2021-05-07 01:22:07</td>\n",
       "      <td>1390477007713091584</td>\n",
       "      <td>CHRISsW0RLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@elonmusk How quickly can we destroy the moon ...</td>\n",
       "      <td>2021-05-07 01:22:07</td>\n",
       "      <td>1390477007469780994</td>\n",
       "      <td>TKO_RS_One_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @MattWallace888: Elon Musk Reveals Mars Dat...</td>\n",
       "      <td>2021-05-07 01:22:05</td>\n",
       "      <td>1390477000658276356</td>\n",
       "      <td>OMEGAoooFIRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @MattWallace888: @nbcsnl The SNL Dogecoin s...</td>\n",
       "      <td>2021-05-07 01:22:05</td>\n",
       "      <td>1390477000515768322</td>\n",
       "      <td>Moe67378043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @Indians: Dogecoin and Franmil home runs ar...</td>\n",
       "      <td>2021-05-07 01:22:05</td>\n",
       "      <td>1390477000138235914</td>\n",
       "      <td>A_Bauer209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @ItsDogeCoin: I’m HODLing. Retweet &amp;amp; pa...</td>\n",
       "      <td>2021-05-07 01:22:04</td>\n",
       "      <td>1390476998758309890</td>\n",
       "      <td>itwascytronical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@FinanceLokum Very Nice Project \\n\\n@Natasha38...</td>\n",
       "      <td>2021-05-07 01:22:04</td>\n",
       "      <td>1390476997227212803</td>\n",
       "      <td>Harsh59680532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @ashwsbreal: 2000 #dogecoin to 5 people Whe...</td>\n",
       "      <td>2021-05-07 01:22:04</td>\n",
       "      <td>1390476994702311429</td>\n",
       "      <td>NenLb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text      tweet_datetime  \\\n",
       "0  RT @TaraBull808: @CEOAdam I'd love to invite y... 2021-05-07 01:22:07   \n",
       "1  RT @Dogecoinrich: What is your #Dogecoin price... 2021-05-07 01:22:07   \n",
       "2  Come check us out, live right now! \\n\\nWe LOVE... 2021-05-07 01:22:07   \n",
       "3  @elonmusk How quickly can we destroy the moon ... 2021-05-07 01:22:07   \n",
       "4  RT @MattWallace888: Elon Musk Reveals Mars Dat... 2021-05-07 01:22:05   \n",
       "5  RT @MattWallace888: @nbcsnl The SNL Dogecoin s... 2021-05-07 01:22:05   \n",
       "6  RT @Indians: Dogecoin and Franmil home runs ar... 2021-05-07 01:22:05   \n",
       "7  RT @ItsDogeCoin: I’m HODLing. Retweet &amp; pa... 2021-05-07 01:22:04   \n",
       "8  @FinanceLokum Very Nice Project \\n\\n@Natasha38... 2021-05-07 01:22:04   \n",
       "9  RT @ashwsbreal: 2000 #dogecoin to 5 people Whe... 2021-05-07 01:22:04   \n",
       "\n",
       "              tweet_Id     Twitter_name  \n",
       "0  1390477009751482376      jamiegagnon  \n",
       "1  1390477008694431746  JustStocksToday  \n",
       "2  1390477007713091584      CHRISsW0RLD  \n",
       "3  1390477007469780994      TKO_RS_One_  \n",
       "4  1390477000658276356     OMEGAoooFIRE  \n",
       "5  1390477000515768322      Moe67378043  \n",
       "6  1390477000138235914       A_Bauer209  \n",
       "7  1390476998758309890  itwascytronical  \n",
       "8  1390476997227212803    Harsh59680532  \n",
       "9  1390476994702311429            NenLb  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('C:/Users/parth/OneDrive/Desktop/twitterfetcheddata.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training and Testing the Given Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-3.1.1-bin-hadoop2.7'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
    "spark = SparkSession.builder.appName(\"TwitterSentimentAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets_csv = pd.read_csv('training_1.csv', names=['label','tweet_id','tweet_date_time','tweet_query','tweet_name','tweet_text'],encoding=\"ISO-8859-1\")\n",
    "tweets_csv.to_csv('training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+----------------------------+-----------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0|label|tweet_id  |tweet_date_time             |tweet_query|tweet_name     |tweet_text                                                                                                         |\n",
      "+---+-----+----------+----------------------------+-----------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |0    |1467810369|Mon Apr 06 22:19:45 PDT 2009|NO_QUERY   |_TheSpecialOne_|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|\n",
      "|1  |0    |1467810672|Mon Apr 06 22:19:49 PDT 2009|NO_QUERY   |scotthamilton  |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |\n",
      "|2  |0    |1467810917|Mon Apr 06 22:19:53 PDT 2009|NO_QUERY   |mattycus       |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |\n",
      "+---+-----+----------+----------------------------+-----------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv file into dataFrame with automatically inferred schema\n",
    "tweets_csv = spark.read.csv('training.csv', inferSchema=True, header=True)\n",
    "tweets_csv.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the related data\n",
    "data = tweets_csv.select(\"label\",\"tweet_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|tweet_text                                                                                                         |SentimentWords                                                                                                                          |\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|[@switchfoot, http://twitpic.com/2y1zl, -, awww,, that's, a, bummer., , you, shoulda, got, david, carr, of, third, day, to, do, it., ;d]|\n",
      "|0    |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |[is, upset, that, he, can't, update, his, facebook, by, texting, it..., and, might, cry, as, a, result, , school, today, also., blah!]  |\n",
      "|0    |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |[@kenichan, i, dived, many, times, for, the, ball., managed, to, save, 50%, , the, rest, go, out, of, bounds]                           |\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Separate \"SentimentText\" into individual words using tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"SentimentWords\")\n",
    "tokenizedTrain = tokenizer.transform(data)\n",
    "tokenizedTrain.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n",
      "|label|tweet_text                                                                                                         |SentimentWords                                                                                                                          |MeaningfulWords                                                                                             |\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n",
      "|0    |@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|[@switchfoot, http://twitpic.com/2y1zl, -, awww,, that's, a, bummer., , you, shoulda, got, david, carr, of, third, day, to, do, it., ;d]|[@switchfoot, http://twitpic.com/2y1zl, -, awww,, bummer., , shoulda, got, david, carr, third, day, it., ;d]|\n",
      "|0    |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |[is, upset, that, he, can't, update, his, facebook, by, texting, it..., and, might, cry, as, a, result, , school, today, also., blah!]  |[upset, update, facebook, texting, it..., might, cry, result, , school, today, also., blah!]                |\n",
      "|0    |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |[@kenichan, i, dived, many, times, for, the, ball., managed, to, save, 50%, , the, rest, go, out, of, bounds]                           |[@kenichan, dived, many, times, ball., managed, save, 50%, , rest, go, bounds]                              |\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Removing stop words (unimportant words to be features)\n",
    "\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
    "SwRemovedTrain.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|MeaningfulWords                                                                                             |features                                                                                                                                                            |\n",
      "+-----+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[@switchfoot, http://twitpic.com/2y1zl, -, awww,, bummer., , shoulda, got, david, carr, third, day, it., ;d]|(262144,[38640,52803,119493,127420,143501,147501,168211,173018,196041,225898,226790,242377,249180,254061],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|0    |[upset, update, facebook, texting, it..., might, cry, result, , school, today, also., blah!]                |(262144,[59577,61031,64238,97598,125635,142393,155889,226565,229007,249180,254543,256468,261626],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])             |\n",
      "|0    |[@kenichan, dived, many, times, ball., managed, save, 50%, , rest, go, bounds]                              |(262144,[3924,28390,95425,148675,152481,178017,199496,220545,230992,235691,245044,249180],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                        |\n",
      "+-----+------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Converting words feature into numerical feature. In Spark 2.2.1,it is implemented in HashingTF funtion using Austin Appleby's MurmurHash 3 algorithm\n",
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "numericTrainData = hashTF.transform(SwRemovedTrain).select(\n",
    "    'label', 'MeaningfulWords', 'features')\n",
    "numericTrainData.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "#Train our classifier model using training data\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", \n",
    "                        maxIter=10, regParam=0.3)\n",
    "model = lr.fit(numericTrainData)\n",
    "print (\"Training is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "tweets_test_csv = pd.read_csv('testdata_1.csv', names=['label','tweet_id','tweet_date_time','tweet_query','tweet_name','tweet_text'],encoding=\"ISO-8859-1\")\n",
    "tweets_test_csv.to_csv('testdata_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+----------------------------+-----------+----------+---------------------------------------------------------------------------------------------------------------+\n",
      "|_c0|label|tweet_id|tweet_date_time             |tweet_query|tweet_name|tweet_text                                                                                                     |\n",
      "+---+-----+--------+----------------------------+-----------+----------+---------------------------------------------------------------------------------------------------------------+\n",
      "|0  |4    |3       |Mon May 11 03:17:40 UTC 2009|kindle2    |tpryan    |@stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.|\n",
      "|1  |4    |4       |Mon May 11 03:18:03 UTC 2009|kindle2    |vcu451    |Reading my kindle2...  Love it... Lee childs is good read.                                                     |\n",
      "|2  |4    |5       |Mon May 11 03:18:54 UTC 2009|kindle2    |chadfu    |Ok, first assesment of the #kindle2 ...it fucking rocks!!!                                                     |\n",
      "+---+-----+--------+----------------------------+-----------+----------+---------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv file into dataFrame with automatically inferred schema\n",
    "tweets_test_csv = spark.read.csv('testdata_1.csv', inferSchema=True, header=True)\n",
    "tweets_test_csv.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select related testing data\n",
    "test_data = tweets_test_csv.select(\"label\",\"tweet_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n",
      "|Label|MeaningfulWords                                                              |features                                                                                                     |\n",
      "+-----+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n",
      "|4    |[@stellargirl, loooooooovvvvvveee, kindle2., dx, cool,, 2, fantastic, right.]|(262144,[7877,12524,83462,122452,158475,167389,199176,258817],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])             |\n",
      "|4    |[reading, kindle2..., , love, it..., lee, childs, good, read.]               |(262144,[32227,73008,113432,129422,186480,193997,198409,226565,249180],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+-----+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prepare testing data\n",
    "tokenizedTest = tokenizer.transform(test_data)\n",
    "SwRemovedTest = swr.transform(tokenizedTest)\n",
    "numericTest = hashTF.transform(SwRemovedTest).select(\n",
    "    'Label', 'MeaningfulWords', 'features')\n",
    "numericTest.show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8209704406023425\n"
     ]
    }
   ],
   "source": [
    "#Predict testing data and calculate the accuracy model\n",
    "prediction = model.transform(numericTest)\n",
    "predictionFinal = prediction.select(\n",
    "    \"MeaningfulWords\", \"prediction\", \"Label\")\n",
    "#predictionFinal.show(n=4, truncate = False)\n",
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
    "totalData = predictionFinal.count()\n",
    "print(\"accuracy:\", correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Predicting the Fetched Twitter Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------+--------------+\n",
      "|_c0|tweet_text                                                                                                                                  |tweet_datetime     |tweet_Id           |Twitter_name  |\n",
      "+---+--------------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------+--------------+\n",
      "|0  |@Dogecoin_1USD @ashwsbreal @elonmusk DL9ut9CFbdQMSQ153NB1VAJietdcDmfZRB thanks!                                                             |2021-05-06 21:16:49|1390415277549133826|michel34901913|\n",
      "|1  |RT @PnetworkC: we just get started @pNetworkDeFi #eidoo #cz_binance #Bitcoin #BNB #PNT #USDT #cryptocurrency  #DeFi #BSCPAD #BSC #Binance #…|2021-05-06 21:16:49|1390415277259739144|76Accuracy    |\n",
      "|2  |RT @bmurphypointman: #Socialmedia #SSM #SEO #makemoney #affiliatemarketing Check my #website #makemoney #earnmoney #affiliate #referral #ca…|2021-05-06 21:16:48|1390415275082801157|twentytenmedia|\n",
      "+---+--------------------------------------------------------------------------------------------------------------------------------------------+-------------------+-------------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv file into dataFrame with automatically inferred schema\n",
    "fetched_csv = spark.read.csv('twitterfetcheddata.csv', inferSchema=True, header=True)\n",
    "fetched_csv.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the fetched data\n",
    "#select related fectched data\n",
    "fetched_data = fetched_csv.select(\"tweet_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tweet_text                                                                                                                                  |MeaningfulWords                                                                                                                                        |features                                                                                                                                                                                                |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|@Dogecoin_1USD @ashwsbreal @elonmusk DL9ut9CFbdQMSQ153NB1VAJietdcDmfZRB thanks!                                                             |[@dogecoin_1usd, @ashwsbreal, @elonmusk, dl9ut9cfbdqmsq153nb1vajietdcdmfzrb, thanks!]                                                                  |(262144,[10726,83680,165296,169435,261172],[1.0,1.0,1.0,1.0,1.0])                                                                                                                                       |\n",
      "|RT @PnetworkC: we just get started @pNetworkDeFi #eidoo #cz_binance #Bitcoin #BNB #PNT #USDT #cryptocurrency  #DeFi #BSCPAD #BSC #Binance #…|[rt, @pnetworkc:, get, started, @pnetworkdefi, #eidoo, #cz_binance, #bitcoin, #bnb, #pnt, #usdt, #cryptocurrency, , #defi, #bscpad, #bsc, #binance, #…]|(262144,[7694,8710,42337,57275,72478,77407,82245,87077,112130,168855,169734,174126,176573,197791,216447,249180,252722,254995],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prepare testing data\n",
    "tokenizedTest_fetch = tokenizer.transform(fetched_data)\n",
    "SwRemovedTest_fetch = swr.transform(tokenizedTest_fetch)\n",
    "numericTest_fetch = hashTF.transform(SwRemovedTest_fetch).select('tweet_text',\n",
    "     'MeaningfulWords', 'features')\n",
    "numericTest_fetch.show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|tweet_text                                                                                                                                  |MeaningfulWords                                                                                                                                         |prediction|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|@Dogecoin_1USD @ashwsbreal @elonmusk DL9ut9CFbdQMSQ153NB1VAJietdcDmfZRB thanks!                                                             |[@dogecoin_1usd, @ashwsbreal, @elonmusk, dl9ut9cfbdqmsq153nb1vajietdcdmfzrb, thanks!]                                                                   |4.0       |\n",
      "|RT @PnetworkC: we just get started @pNetworkDeFi #eidoo #cz_binance #Bitcoin #BNB #PNT #USDT #cryptocurrency  #DeFi #BSCPAD #BSC #Binance #…|[rt, @pnetworkc:, get, started, @pnetworkdefi, #eidoo, #cz_binance, #bitcoin, #bnb, #pnt, #usdt, #cryptocurrency, , #defi, #bscpad, #bsc, #binance, #…] |0.0       |\n",
      "|RT @bmurphypointman: #Socialmedia #SSM #SEO #makemoney #affiliatemarketing Check my #website #makemoney #earnmoney #affiliate #referral #ca…|[rt, @bmurphypointman:, #socialmedia, #ssm, #seo, #makemoney, #affiliatemarketing, check, #website, #makemoney, #earnmoney, #affiliate, #referral, #ca…]|0.0       |\n",
      "|@MindMathMoney If Dogecoin can reach 0.70 Cardano can do $ 5.                                                                               |[@mindmathmoney, dogecoin, reach, 0.70, cardano, $, 5.]                                                                                                 |0.0       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predict fetched data\n",
    "prediction = model.transform(numericTest_fetch)\n",
    "predictionFinal_fetch = prediction.select(\"tweet_text\",\"MeaningfulWords\", \"prediction\")\n",
    "predictionFinal_fetch.show(n=4, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save the Fetched data and Classification Result Into SQL Database</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the fetched data and classification result into SQL Database\n",
    "#set variable to be used to connect the database\n",
    "database = \"TwitterDB\"\n",
    "table = \"dbo.tbl_spark_df\"\n",
    "user = \"user\"\n",
    "password  = \"\"\n",
    " \n",
    "#write the dataframe into a sql table\n",
    "predictionFinal_fetch.write.mode(\"overwrite\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
